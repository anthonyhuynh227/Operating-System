Question #1
Why might an application prefer using malloc and free instead of using sbrk directly? What is the relationship between malloc/free and sbkr?
An application might prefer using malloc and free instead of sbrk() directly because it provides a cleaner interface. If you want to efficiently use memory, you’ll want to reuse memory whenever possible, and malloc/free provides a relatively simple interface for doing so. For one, malloc() and free() hides away all of the implementation details of managing the explicit free list and managing which parts of memory are being used or available to use. Malloc() also takes care of aligning the pointer to match the alignment of any variable. 

On the other hand, using sbrk() on its own is a bit more complicated. If you are going to reuse memory whenever possible, you will have to keep track of which parts of memory are used and which parts are free. That means you’ll need to keep track of your own data structures with this information and update them whenever necessary. Furthermore, you’ll also have to know when to call sbrk() to increase the data segment when necessary. This makes it very difficult to manage the dynamic memory of your program.

The relationship between malloc/free and sbrk() is as follows. Malloc() manages an explicit free list, which contains free blocks with a Header metadata. A Header contains a pointer to the next free block and a size field of the free block. When a user calls malloc(), the function will search this free list to find the first free block with enough size to fulfill the user’s demands; if no such block can be found, the function will call morecore(), which uses sbrk() to allocate enough additional memory in the data segment to fulfill the user’s request. Then, a pointer to this additional memory is returned to the user.
Question #2:
Explain how the file descriptors are set up by the xk's shell program in order to run the command ls | wc.
When the shell first starts, it creates the three initial file descriptors for stdin, stdout, and stderr. When the command ls | wc runs, the shell will do the following:
Fork a process, say A, to run the ls | wc command.
Process A calls pipe() to create a pipe with a read-end and a write-end file descriptor. 
Process A forks a process B to run the ls command. Process B will close the stdout (1) fd and duplicate the write end of the pipe. This will ensure that the 1 fd now refers to the write end of the pipe. Close the original descriptors for the pipe, since they should not be used by the ls command. Then, exec() the ls command on process B.
Process A forks a process C to run the wc command. Process C will close the stdin (0) fd and duplicate the read end of the pipe. This will ensure that the 0 fd now refers to the read end of the pipe. Close the original descriptors for the pipe, since they should not be used by the wc command. Then, exec() the wc command on process C.
The original process A now closes the original pipe descriptors, waits for both B and C to complete, then exits.
Now, we see that ls will send its data to stdout, which is the pipe write end. The wc command will also read its data from stdin, which is the pipe read end. The pipe enables the command to essentially get the word count of the output of ls.
Question #3:
When a syscall completes, user-level execution resumes with the instruction immediately after the syscall. When a page fault exception completes, where does user-level execution resume?
Assuming that the page fault was an expected one (such as a COW or swap space fault), the user-level execution resumes the same instruction that causes the page fault exception. This is necessary because otherwise we would be skipping over an instruction.
Question #4:
How should the kernel decide whether an unmapped reference is a normal stack operation versus a stray pointer dereference that should cause the application to halt? What should happen, for example, if an application calls a procedure with a local variable that is an array of a million integers?
The kernel might decide to cap the stack size given to a process. This is what xk does, as it caps the stack at 10 pages. The kernel can always check whether an unmapped reference is within the bounds of the maximum stack size, and only allocate stack pages if it is within range. 
If an application tries to put an array of a million integers onto the stack, it should only succeed if it does not exceed the max stack size. In xk, this will obviously fail, since 10 pages of 4096 bytes each is nowhere near enough. The xk kernel will check for this on page faults and will kill the process if it oversteps.
Question #5:
Is it possible to reduce the user stack size at run-time (i.e., to deallocate the user stack when a procedure with a large number of local variables goes out of scope)? If so, sketch how that might work.
In principle, this is possible. For a process, the stack pointer determines the size of the stack that is currently being used; it is the difference between the stack base and the stack pointer. At the same time, the kernel tracks how many stack pages are allocated to each process with internal data structures (such as the stack vregion is xk). 
One might implement stack deallocation as follows. First, after some regular event for each process (such as a timer interrupt), find the current amount of stack being used by taking the difference between the stack base and the stack pointer. Also, find how many stack pages to the process are allocated in memory. If the difference is significant, then we can deallocate the excess stack pages that are not being used (of course, making sure to update all relevant data structures and pushing the new pages tables to the CPU). 
Question #6:
The TLB caches the page table entries of recently referenced pages. When you modify the page table entry to allow write access, which function should you use in xk ensure that the TLB does not have a stale version of the cache? How does it work?
We use the function vspaceinvalidate() to update the page table and then we use vspaceinstall() to install the most updated version of the page table to the CPU. In particular, the function vspaceinstall() will flush the TLB in x86-64 and ensure that it does not have a stale version. 
The function works as follows. First, it turns off all interrupts so it cannot be interrupted in the middle of installing the pages tables. Then, it simply copies the pointer to the base page directory to the register CR3, which is supposed to hold the pointer to the page directory. Writing to the CR3 register automatically flushes the TLB. Then, it re-enables interrupts and returns. 
Question #7
For each member of the project team, how many hours did you spend on this lab?
We each spent about 10 hours in the lab.
Question #8
What did you like or dislike about this lab? Is there anything you wish you knew earlier?
Our favorite part of the lab was enabling the shell program and seeing it run. It was very cool to see all of the work we did for system calls, the file descriptors, etc. be used in a familiar program like the shell. It was also fun to be able to view the internals of shell command programs like ls and wc, and to see how exactly they work using the operating system interface.

We disliked the opaqueness of some of the important vspace functions, namely vspaceinstall(). We wish that the lab could have specified that vspaceinstall() explicitly flushes the TLB cache. Initially, we assumed that only changing the page tables with vspaceinvalidate() was enough, and that this would flush the TLB on its own. This caused a large number of bugs in our lab, which could have been avoided if we knew the exact functionality of vspaceinstall(). Overall, we wish that we knew earlier how the vspace was designed. This includes the functions as well as the data structures used, such as the linked list structure of vpi_pages in conjunction with the internal arrays of vpage_infos.
